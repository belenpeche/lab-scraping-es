{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Web-Scraping-Lab\" data-toc-modified-id=\"Web-Scraping-Lab-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Web Scraping Lab</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Useful-Resources\" data-toc-modified-id=\"Useful-Resources-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Useful Resources</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-of-all,-gathering-our-tools.\" data-toc-modified-id=\"First-of-all,-gathering-our-tools.-1.0.1.1\"><span class=\"toc-item-num\">1.0.1.1&nbsp;&nbsp;</span>First of all, gathering our tools.</a></span></li><li><span><a href=\"#Challenge-1---Download,-parse-(using-BeautifulSoup),-and-print-the-content-from-the-Trending-Developers-page-from-GitHub:\" data-toc-modified-id=\"Challenge-1---Download,-parse-(using-BeautifulSoup),-and-print-the-content-from-the-Trending-Developers-page-from-GitHub:-1.0.1.2\"><span class=\"toc-item-num\">1.0.1.2&nbsp;&nbsp;</span>Challenge 1 - Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:</a></span></li><li><span><a href=\"#Display-the-names-of-the-trending-developers-retrieved-in-the-previous-step.\" data-toc-modified-id=\"Display-the-names-of-the-trending-developers-retrieved-in-the-previous-step.-1.0.1.3\"><span class=\"toc-item-num\">1.0.1.3&nbsp;&nbsp;</span>Display the names of the trending developers retrieved in the previous step.</a></span></li><li><span><a href=\"#Challenge-2---Display-the-trending-Python-repositories-in-GitHub\" data-toc-modified-id=\"Challenge-2---Display-the-trending-Python-repositories-in-GitHub-1.0.1.4\"><span class=\"toc-item-num\">1.0.1.4&nbsp;&nbsp;</span>Challenge 2 - Display the trending Python repositories in GitHub</a></span></li><li><span><a href=\"#Challenge-3---Display-all-the-image-links-from-Walt-Disney-wikipedia-page\" data-toc-modified-id=\"Challenge-3---Display-all-the-image-links-from-Walt-Disney-wikipedia-page-1.0.1.5\"><span class=\"toc-item-num\">1.0.1.5&nbsp;&nbsp;</span>Challenge 3 - Display all the image links from Walt Disney wikipedia page</a></span></li><li><span><a href=\"#Challenge-4---Retrieve-all-links-to-pages-on-Wikipedia-that-refer-to-some-kind-of-Python.\" data-toc-modified-id=\"Challenge-4---Retrieve-all-links-to-pages-on-Wikipedia-that-refer-to-some-kind-of-Python.-1.0.1.6\"><span class=\"toc-item-num\">1.0.1.6&nbsp;&nbsp;</span>Challenge 4 - Retrieve all links to pages on Wikipedia that refer to some kind of Python.</a></span></li><li><span><a href=\"#Challenge-5---Number-of-Titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point\" data-toc-modified-id=\"Challenge-5---Number-of-Titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point-1.0.1.7\"><span class=\"toc-item-num\">1.0.1.7&nbsp;&nbsp;</span>Challenge 5 - Number of Titles that have changed in the United States Code since its last release point</a></span></li><li><span><a href=\"#Challenge-6---A-Python-list-with-the-top-ten-FBI's-Most-Wanted-names\" data-toc-modified-id=\"Challenge-6---A-Python-list-with-the-top-ten-FBI's-Most-Wanted-names-1.0.1.8\"><span class=\"toc-item-num\">1.0.1.8&nbsp;&nbsp;</span>Challenge 6 - A Python list with the top ten FBI's Most Wanted names</a></span></li><li><span><a href=\"#Challenge-7---List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org\" data-toc-modified-id=\"Challenge-7---List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org-1.0.1.9\"><span class=\"toc-item-num\">1.0.1.9&nbsp;&nbsp;</span>Challenge 7 - List all language names and number of related articles in the order they appear in wikipedia.org</a></span></li><li><span><a href=\"#Challenge-8---A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk\" data-toc-modified-id=\"Challenge-8---A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk-1.0.1.10\"><span class=\"toc-item-num\">1.0.1.10&nbsp;&nbsp;</span>Challenge 8 - A list with the different kind of datasets available in data.gov.uk</a></span></li><li><span><a href=\"#Challenge-9---Top-10-languages-by-number-of-native-speakers-stored-in-a-Pandas-Dataframe\" data-toc-modified-id=\"Challenge-9---Top-10-languages-by-number-of-native-speakers-stored-in-a-Pandas-Dataframe-1.0.1.11\"><span class=\"toc-item-num\">1.0.1.11&nbsp;&nbsp;</span>Challenge 9 - Top 10 languages by number of native speakers stored in a Pandas Dataframe</a></span></li></ul></li><li><span><a href=\"#Stepping-up-the-game\" data-toc-modified-id=\"Stepping-up-the-game-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Stepping up the game</a></span><ul class=\"toc-item\"><li><span><a href=\"#Challenge-10---The-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe\" data-toc-modified-id=\"Challenge-10---The-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe-1.0.2.1\"><span class=\"toc-item-num\">1.0.2.1&nbsp;&nbsp;</span>Challenge 10 - The 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe</a></span></li><li><span><a href=\"#Challenge-11---IMDB's-Top-250-data-(movie-name,-Initial-release,-director-name-and-stars)-as-a-pandas-dataframe\" data-toc-modified-id=\"Challenge-11---IMDB's-Top-250-data-(movie-name,-Initial-release,-director-name-and-stars)-as-a-pandas-dataframe-1.0.2.2\"><span class=\"toc-item-num\">1.0.2.2&nbsp;&nbsp;</span>Challenge 11 - IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe</a></span></li><li><span><a href=\"#Challenge-12---Movie-name,-year-and-a-brief-summary-of-the-top-10-random-movies-(IMDB)-as-a-pandas-dataframe.\" data-toc-modified-id=\"Challenge-12---Movie-name,-year-and-a-brief-summary-of-the-top-10-random-movies-(IMDB)-as-a-pandas-dataframe.-1.0.2.3\"><span class=\"toc-item-num\">1.0.2.3&nbsp;&nbsp;</span>Challenge 12 - Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe.</a></span></li><li><span><a href=\"#Challenge-13---Find-the-live-weather-report-(temperature,-wind-speed,-description-and-weather)-of-a-given-city.\" data-toc-modified-id=\"Challenge-13---Find-the-live-weather-report-(temperature,-wind-speed,-description-and-weather)-of-a-given-city.-1.0.2.4\"><span class=\"toc-item-num\">1.0.2.4&nbsp;&nbsp;</span>Challenge 13 - Find the live weather report (temperature, wind speed, description and weather) of a given city.</a></span></li><li><span><a href=\"#Challenge-14---Book-name,price-and-stock-availability-as-a-pandas-dataframe.\" data-toc-modified-id=\"Challenge-14---Book-name,price-and-stock-availability-as-a-pandas-dataframe.-1.0.2.5\"><span class=\"toc-item-num\">1.0.2.5&nbsp;&nbsp;</span>Challenge 14 - Book name,price and stock availability as a pandas dataframe.</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio de Web Scraping\n",
    "\n",
    "EncontrarÃ¡s en este cuaderno algunos ejercicios de web scraping para practicar tus habilidades de scraping usando `requests` y `Beautiful Soup`.\n",
    "\n",
    "**Consejos:**\n",
    "\n",
    "- Verifica el [cÃ³digo de estado de la respuesta](https://http.cat/) para cada solicitud para asegurarte de haber obtenido el contenido previsto.\n",
    "- Observa el cÃ³digo HTML en cada solicitud para entender el tipo de informaciÃ³n que estÃ¡s obteniendo y su formato.\n",
    "- Busca patrones en el texto de respuesta para extraer los datos/informaciÃ³n solicitados en cada pregunta.\n",
    "- Visita cada URL y echa un vistazo a su fuente a travÃ©s de Chrome DevTools. NecesitarÃ¡s identificar las etiquetas HTML, nombres de clases especiales, etc., utilizados para el contenido HTML que se espera extraer.\n",
    "- Revisa los selectores CSS.\n",
    "\n",
    "### Recursos Ãštiles\n",
    "- DocumentaciÃ³n de la [biblioteca Requests](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Doc de Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Lista de cÃ³digos de estado HTTP](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [Conceptos bÃ¡sicos de HTML](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [Conceptos bÃ¡sicos de CSS](https://www.cssbasics.com/#page_start)\n",
    "\n",
    "#### Primero que todo, reuniendo nuestras herramientas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš ï¸ **Nuevamente, recuerda limitar tu salida antes de la entrega para que tu cÃ³digo no se pierda en la salida.**\n",
    "\n",
    "#### DesafÃ­o 1 - Descargar, analizar (usando BeautifulSoup) e imprimir el contenido de la pÃ¡gina de Desarrolladores en Tendencia de GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_html=requests.get(url).text\n",
    "soup = BeautifulSoup(github_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muestra los nombres de los desarrolladores en tendencia recuperados en el paso anterior.\n",
    "\n",
    "Tu salida debe ser una lista de Python con los nombres de los desarrolladores. Cada nombre no debe contener ninguna etiqueta HTML.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. Descubre la etiqueta HTML y los nombres de clase usados para los nombres de los desarrolladores. Puedes lograr esto usando Chrome DevTools.\n",
    "\n",
    "1. Usa BeautifulSoup para extraer todos los elementos HTML que contienen los nombres de los desarrolladores.\n",
    "\n",
    "1. Utiliza tÃ©cnicas de manipulaciÃ³n de cadenas para reemplazar espacios en blanco y saltos de lÃ­nea (es decir, `\\n`) en el *texto* de cada elemento HTML. Usa una lista para almacenar los nombres limpios.\n",
    "\n",
    "1. Imprime la lista de nombres.\n",
    "\n",
    "Tu salida deberÃ­a lucir como abajo (con nombres diferentes):\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (ç¥žæ¥½å‚è¦šã€…)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sebastian Raschka', 'LangChain4j', 'Norman Maurer', 'doronz88', 'JJ Kasper', 'Miguel Beltran', 'Vik Paruchuri', 'Arthur', 'Holt Skinner', 'Huang Huang', 'Aliaksandr Valialkin', 'Massimiliano Pippi', 'Jack Lloyd', 'Rich Harris', 'Tobias Klauser', 'Lukas Masuch', 'Charlie Marsh', 'Steven Silvester', 'Matthias Seitz', 'Justin Lebar', 'Mattias Wadman', 'David Sherret', 'Gal Schlezinger', 'Mitchell Hashimoto', 'awaelchli']\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(github_html, \"html.parser\")\n",
    "\n",
    "developers = soup.find_all('h1', class_='h3 lh-condensed')\n",
    "\n",
    "developer_names = [dev.get_text(strip=True) for dev in developers]\n",
    "\n",
    "print(developer_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DesafÃ­o 2 - Mostrar los repositorios de Python en tendencia en GitHub\n",
    "\n",
    "Los pasos para resolver este problema son similares al anterior, excepto que necesitas encontrar los nombres de los repositorios en lugar de los nombres de los desarrolladores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url2 = 'https://github.com/trending/python?since=daily'\n",
    "datos2 = requests.get(f\"{url2}\").text\n",
    "soup2 = BeautifulSoup(datos2, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "repositories = soup.find_all('h1', class_='h3 lh-condensed')\n",
    "\n",
    "repository_names = [repo.get_text(strip=True) for repo in repositories]\n",
    "\n",
    "print(repository_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DesafÃ­o 3 - Mostrar todos los enlaces de imÃ¡genes de la pÃ¡gina de Wikipedia de Walt Disney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url3 = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "disney = requests.get(f\"{url3}\").text\n",
    "soup3 = BeautifulSoup(disney, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https:/static/images/icons/wikipedia.png', 'https:/static/images/mobile/copyright/wikipedia-wordmark-en.svg', 'https:/static/images/mobile/copyright/wikipedia-tagline-en.svg', 'https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG', 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg/220px-Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1b/Nuvola_apps_kaboodle.svg/16px-Nuvola_apps_kaboodle.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg', 'https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Disney_Oscar_1953_%28cropped%29.jpg/170px-Disney_Oscar_1953_%28cropped%29.jpg', 'https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/20px-Commons-logo.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/23px-Wikiquote-logo.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/26px-Wikisource-logo.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/9/96/Symbol_category_class.svg/16px-Symbol_category_class.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/135px-Disneyland_Resort_logo.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/20px-Animation_disc.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/19px-P_vip.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Mickey_Mouse_colored_%28head%29.svg/20px-Mickey_Mouse_colored_%28head%29.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/19px-Video-x-generic.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/21px-Flag_of_Los_Angeles_County%2C_California.svg.png', 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/21px-Blank_television_set.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/21px-Flag_of_the_United_States.svg.png', 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png', 'https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1', 'https:/static/images/footer/wikimedia-button.png', 'https:/static/images/footer/poweredby_mediawiki_88x31.png']\n"
     ]
    }
   ],
   "source": [
    "img_tags = soup3.find_all('img')\n",
    "\n",
    "img_links = []\n",
    "base_url = \"https:\" \n",
    "\n",
    "for img in img_tags:\n",
    "    img_src = img.get('src')\n",
    "    if img_src:\n",
    "        if not img_src.startswith(\"http\"):\n",
    "            img_src = base_url + img_src\n",
    "        img_links.append(img_src)\n",
    "\n",
    "print(img_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DesafÃ­o 4 - Recuperar todos los enlaces a pÃ¡ginas en Wikipedia que se refieren a algÃºn tipo de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "url4 ='https://en.wikipedia.org/wiki/Python' \n",
    "python = requests.get(f\"{url4}\").text\n",
    "soup4 = BeautifulSoup(python, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&returnto=Python', 'https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Python', 'https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&returnto=Python', 'https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Python', 'https://af.wikipedia.org/wiki/Python', 'https://als.wikipedia.org/wiki/Python', 'https://az.wikipedia.org/wiki/Python_(d%C9%99qiql%C9%99%C5%9Fdirm%C9%99)', 'https://be.wikipedia.org/wiki/Python', 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)', 'https://da.wikipedia.org/wiki/Python', 'https://de.wikipedia.org/wiki/Python', 'https://eu.wikipedia.org/wiki/Python_(argipena)', 'https://fr.wikipedia.org/wiki/Python', 'https://hr.wikipedia.org/wiki/Python_(razdvojba)', 'https://id.wikipedia.org/wiki/Python', 'https://ia.wikipedia.org/wiki/Python_(disambiguation)', 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)', 'https://it.wikipedia.org/wiki/Python_(disambigua)', 'https://la.wikipedia.org/wiki/Python_(discretiva)', 'https://lb.wikipedia.org/wiki/Python', 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)', 'https://nl.wikipedia.org/wiki/Python', 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)', 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)', 'https://sk.wikipedia.org/wiki/Python', 'https://sh.wikipedia.org/wiki/Python', 'https://fi.wikipedia.org/wiki/Python', 'https://tr.wikipedia.org/wiki/Python_(anlam_ayr%C4%B1m%C4%B1)', 'https://vi.wikipedia.org/wiki/Python', 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)', 'https://en.wikipedia.org/wiki/Python', 'https://en.wikipedia.org/wiki/Talk:Python', 'https://en.wikipedia.org/wiki/Python', 'https://en.wikipedia.org/w/index.php?title=Python&action=edit', 'https://en.wikipedia.org/w/index.php?title=Python&action=history', 'https://en.wikipedia.org/wiki/Python', 'https://en.wikipedia.org/w/index.php?title=Python&action=edit', 'https://en.wikipedia.org/w/index.php?title=Python&action=history', 'https://en.wikipedia.org/wiki/Special:WhatLinksHere/Python', 'https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Python', 'https://en.wikipedia.org/w/index.php?title=Python&oldid=1213626445', 'https://en.wikipedia.org/w/index.php?title=Python&action=info', 'https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&page=Python&id=1213626445&wpFormIdentifier=titleform', 'https://en.wikipedia.org/w/index.php?title=Special:UrlShortener&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FPython', 'https://en.wikipedia.org/w/index.php?title=Special:QrCode&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FPython', 'https://en.wikipedia.org/w/index.php?title=Special:DownloadAsPdf&page=Python&action=show-download-screen', 'https://en.wikipedia.org/w/index.php?title=Python&printable=yes', 'https://commons.wikimedia.org/wiki/Category:Python', 'https://en.wiktionary.org/wiki/Python', 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=1', 'https://en.wikipedia.org/wiki/Pythonidae', 'https://en.wikipedia.org/wiki/Python_(genus)', 'https://en.wikipedia.org/wiki/Python_(mythology)', 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=2', 'https://en.wikipedia.org/wiki/Python_(programming_language)', 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=3', 'https://en.wikipedia.org/wiki/Python_of_Aenus', 'https://en.wikipedia.org/wiki/Python_(painter)', 'https://en.wikipedia.org/wiki/Python_of_Byzantium', 'https://en.wikipedia.org/wiki/Python_of_Catana', 'https://en.wikipedia.org/wiki/Python_Anghelo', 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=4', 'https://en.wikipedia.org/wiki/Python_(Efteling)', 'https://en.wikipedia.org/wiki/Python_(Busch_Gardens_Tampa_Bay)', 'https://en.wikipedia.org/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)', 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=5', 'https://en.wikipedia.org/wiki/Python_(automobile_maker)', 'https://en.wikipedia.org/wiki/Python_(Ford_prototype)', 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=6', 'https://en.wikipedia.org/wiki/Python_(missile)', 'https://en.wikipedia.org/wiki/Python_(nuclear_primary)', 'https://en.wikipedia.org/wiki/Colt_Python', 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=7', 'https://en.wikipedia.org/wiki/Python_(codename)', 'https://en.wikipedia.org/wiki/Python_(film)', 'https://en.wikipedia.org/wiki/Monty_Python', 'https://en.wikipedia.org/wiki/Python_(Monty)_Pictures', 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=8', 'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0', 'https://en.wikipedia.org/w/index.php?title=Python&oldid=1213626445', 'https://en.wikipedia.org//en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile']\n"
     ]
    }
   ],
   "source": [
    "a_tags = soup4.find_all('a', href=True)\n",
    "\n",
    "python_links = []\n",
    "base_url = \"https://en.wikipedia.org\"\n",
    "\n",
    "for tag in a_tags:\n",
    "    href = tag['href']\n",
    "    if 'Python' in href:\n",
    "        if not href.startswith(\"http\"):\n",
    "            href = base_url + href\n",
    "        python_links.append(href)\n",
    "\n",
    "print(python_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DesafÃ­o 5 - NÃºmero de TÃ­tulos que han cambiado en el CÃ³digo de los Estados Unidos desde su Ãºltimo punto de lanzamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'\n",
    "uscode_html = requests.get(url).text\n",
    "soup = BeautifulSoup(uscode_html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NÃºmero de TÃ­tulos que han cambiado: 0\n"
     ]
    }
   ],
   "source": [
    "changed_titles_section = soup.find('div', class_='usctitlechanged')\n",
    "\n",
    "if changed_titles_section:\n",
    "    changed_titles = changed_titles_section.find_all('div', class_='uscitemlist')\n",
    "    number_of_changed_titles = len(changed_titles)\n",
    "else:\n",
    "    number_of_changed_titles = 0\n",
    "\n",
    "print(f'NÃºmero de TÃ­tulos que han cambiado: {number_of_changed_titles}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DesafÃ­o 6 - Una lista de Python con los diez nombres mÃ¡s buscados por el FBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url7 = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted = requests.get(f\"{url7}\").text\n",
    "soup7 = BeautifulSoup(wanted, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "wanted_tags = soup7.find_all('h3', class_='title')\n",
    "\n",
    "wanted_names = [tag.get_text(strip=True) for tag in wanted_tags]\n",
    "\n",
    "top_ten_wanted_names = wanted_names[:10]\n",
    "\n",
    "print(top_ten_wanted_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DesafÃ­o 7 - Listar todos los nombres de idiomas y el nÃºmero de artÃ­culos relacionados en el orden en que aparecen en wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url8 = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = requests.get(f\"{url8}\").text\n",
    "soup8 = BeautifulSoup(languages, 'html.parser')\n",
    "langlist = soup8.find_all(\"div\", {\"class\": f\"central-featured-lang\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: 6,796,000+articles\n",
      "ÃÂ Ã‘ÂƒÃ‘ÂÃ‘ÂÃÂºÃÂ¸ÃÂ¹: 1Ã‚Â 969Ã‚Â 000+Ã‘ÂÃ‘Â‚ÃÂ°Ã‘Â‚ÃÂµÃÂ¹\n",
      "Ã¦Â—Â¥Ã¦ÂœÂ¬Ã¨ÂªÂž: 1,407,000+Ã¨Â¨Â˜Ã¤ÂºÂ‹\n",
      "EspaÃƒÂ±ol: 1.938.000+artÃƒÂ­culos\n",
      "Deutsch: 2.891.000+Artikel\n",
      "FranÃƒÂ§ais: 2Ã¢Â€Â¯598Ã¢Â€Â¯000+articles\n",
      "Italiano: 1.853.000+voci\n",
      "Ã¤Â¸Â­Ã¦Â–Â‡: 1,409,000+Ã¦ÂÂ¡Ã§Â›Â® / Ã¦Â¢ÂÃ§Â›Â®\n",
      "Ã™ÂÃ˜Â§Ã˜Â±Ã˜Â³Ã›ÂŒ: Ã›Â¹Ã›Â¹Ã›ÂµÃ™Â¬Ã›Â°Ã›Â°Ã›Â°+Ã™Â…",
      "Ã™Â‚Ã˜Â§Ã™Â„Ã™Â‡\n",
      "PortuguÃƒÂªs: 1.120.000+artigos\n"
     ]
    }
   ],
   "source": [
    "languages_info = []\n",
    "\n",
    "for lang in langlist:\n",
    "    language_name = lang.find(\"strong\").get_text(strip=True)\n",
    "    article_count = lang.find(\"small\").get_text(strip=True)\n",
    "    languages_info.append((language_name, article_count))\n",
    "\n",
    "for language, count in languages_info:\n",
    "    print(f\"{language}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DesafÃ­o 8 - Una lista con los diferentes tipos de conjuntos de datos disponibles en data.gov.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url82 = 'https://data.gov.uk/'\n",
    "dats = requests.get(f\"{url82}\")\n",
    "soup8 = BeautifulSoup(dats.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cookies to collect information', 'View cookies', 'change your cookie settings', 'feedback', 'Business and economy', 'Crime and justice', 'Defence', 'Education', 'Environment', 'Government', 'Government spending', 'Health', 'Mapping', 'Society', 'Towns and cities', 'Transport', 'Digital service performance', 'Government reference data']\n"
     ]
    }
   ],
   "source": [
    "dataset_links = soup8.find_all('a', class_='govuk-link')\n",
    "\n",
    "dataset_types = [link.get_text(strip=True) for link in dataset_links]\n",
    "\n",
    "unique_dataset_types = list(dict.fromkeys(dataset_types))\n",
    "\n",
    "print(unique_dataset_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DesafÃ­o 9 - Los 10 idiomas con mÃ¡s hablantes nativos almacenados en un DataFrame de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url9 = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "tenlang = requests.get(url9)\n",
    "soup9 = BeautifulSoup(tenlang.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Language Native speakers(in millions) Language family        Branch\n",
      "0  Mandarin Chinese                          941    Sino-Tibetan       Sinitic\n",
      "1           Spanish                          486   Indo-European       Romance\n",
      "2           English                          380   Indo-European      Germanic\n",
      "3             Hindi                          345   Indo-European    Indo-Aryan\n",
      "4           Bengali                          237   Indo-European    Indo-Aryan\n",
      "5        Portuguese                          236   Indo-European       Romance\n",
      "6           Russian                          148   Indo-European  Balto-Slavic\n",
      "7          Japanese                          123         Japonic      Japanese\n",
      "8       Yue Chinese                           86    Sino-Tibetan       Sinitic\n",
      "9        Vietnamese                           85   Austroasiatic        Vietic\n"
     ]
    }
   ],
   "source": [
    "table = soup9.find('table', class_='wikitable')\n",
    "\n",
    "data = []\n",
    "rows = table.find_all('tr')\n",
    "for row in rows:\n",
    "    cols = row.find_all(['th', 'td'])\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    data.append(cols)\n",
    "\n",
    "df = pd.DataFrame(data[1:], columns=data[0])\n",
    "\n",
    "top_10_languages = df.head(10)\n",
    "\n",
    "print(top_10_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subiendo el nivel\n",
    "#### DesafÃ­o 10 - La informaciÃ³n de los 20 Ãºltimos terremotos (fecha, hora, latitud, longitud y nombre de la regiÃ³n) por el EMSC como un dataframe de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "#url7 = 'https://www.emsc-csem.org/Earthquake/'\n",
    "url7 = \"https://www.emsc-csem.org/#2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Fecha, Hora, Latitud, Longitud, Profundidad, Magnitud, RegiÃ³n]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url7)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "table = soup.find('tbody')\n",
    "\n",
    "data = []\n",
    "rows = table.find_all('tr')\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    data.append(cols)\n",
    "\n",
    "columns = ['Fecha', 'Hora', 'Latitud', 'Longitud', 'Profundidad', 'Magnitud', 'RegiÃ³n']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "last_20_earthquakes = df.head(20)\n",
    "\n",
    "print(last_20_earthquakes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DesafÃ­o 11 - Datos del Top 250 de IMDB (nombre de la pelÃ­cula, lanzamiento inicial, nombre del director y estrellas) como un dataframe de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url11 = 'https://www.imdb.com/chart/top'\n",
    "pelis = requests.get(url11)\n",
    "soup = BeautifulSoup(pelis.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m table \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtbody\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlister-list\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 4\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m rows:\n\u001b[1;32m      6\u001b[0m     cols \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "table = soup.find('tbody', class_='lister-list')\n",
    "\n",
    "data = []\n",
    "rows = table.find_all('tr')\n",
    "for row in rows:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    data.append(cols)\n",
    "\n",
    "columns = ['Nombre', 'AÃ±o de Lanzamiento', 'Director', 'Estrellas']\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "top_250_movies = df.head(250)\n",
    "\n",
    "print(top_250_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DesafÃ­o 12 - Nombre de la pelÃ­cula, aÃ±o y un breve resumen de las 10 pelÃ­culas aleatorias top (IMDB) como un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(soup.find_all(\"span\", {\"class\": \"secondaryInfo\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random_movies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m movie_link \u001b[38;5;129;01min\u001b[39;00m \u001b[43mrandom_movies\u001b[49m:\n\u001b[1;32m      2\u001b[0m     movie_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://www.imdb.com\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m movie_link[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m     movie_response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(movie_url)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random_movies' is not defined"
     ]
    }
   ],
   "source": [
    "for movie_link in random_movies:\n",
    "    movie_url = 'http://www.imdb.com' + movie_link['href']\n",
    "    movie_response = requests.get(movie_url)\n",
    "    movie_soup = BeautifulSoup(movie_response.content, 'html.parser')\n",
    "\n",
    "    title = movie_soup.find('h1').text.strip()\n",
    "\n",
    "    year = movie_soup.find('span', class_='TitleBlockMetaData__ListItemText-sc-12ein40-2 jedhex').text.strip()\n",
    "    \n",
    "    summary = movie_soup.find('span', class_='GenresAndPlot__TextContainerBreakpointXL-cum89p-2 dcFkRD').text.strip()\n",
    "\n",
    "    movie_data.append({'Nombre': title, 'AÃ±o': year, 'Resumen': summary})\n",
    "\n",
    "df = pd.DataFrame(movie_data)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DesafÃ­o 13 - Encontrar el reporte meteorolÃ³gico en vivo (temperatura, velocidad del viento, descripciÃ³n y clima) de una ciudad dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: No se pudo obtener el reporte meteorolÃ³gico.\n"
     ]
    }
   ],
   "source": [
    "def weather(city):\n",
    "    api_key = 'YOUR_API_KEY'\n",
    "    base_url = f'http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric'\n",
    "    response = requests.get(base_url)\n",
    "    data = response.json()\n",
    "\n",
    "    if data['cod'] == 200:\n",
    "        weather_data = {\n",
    "            'Ciudad': data['name'],\n",
    "            'Temperatura': data['main']['temp'],\n",
    "            'Velocidad del Viento': data['wind']['speed'],\n",
    "            'DescripciÃ³n': data['weather'][0]['description'],\n",
    "            'Clima': data['weather'][0]['main']\n",
    "        }\n",
    "        return weather_data\n",
    "    else:\n",
    "        return \"Error: No se pudo obtener el reporte meteorolÃ³gico.\"\n",
    "\n",
    "city = 'London'\n",
    "print(weather(city))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DesafÃ­o 14 - Nombre del libro, precio y disponibilidad de stock como un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url14 = 'http://books.toscrape.com/'\n",
    "books = requests.get(url11)\n",
    "soup = BeautifulSoup(books.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libros = list(soup.select('h1.h3 a[tittle]'))\n",
    "#nombres = soup.select('h1.h3 a[tittle]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Nombre del Libro  Precio Disponibilidad\n",
      "0                                A Light in the Attic  Â£51.77       In stock\n",
      "1                                  Tipping the Velvet  Â£53.74       In stock\n",
      "2                                          Soumission  Â£50.10       In stock\n",
      "3                                       Sharp Objects  Â£47.82       In stock\n",
      "4               Sapiens: A Brief History of Humankind  Â£54.23       In stock\n",
      "5                                     The Requiem Red  Â£22.65       In stock\n",
      "6   The Dirty Little Secrets of Getting Your Dream...  Â£33.34       In stock\n",
      "7   The Coming Woman: A Novel Based on the Life of...  Â£17.93       In stock\n",
      "8   The Boys in the Boat: Nine Americans and Their...  Â£22.60       In stock\n",
      "9                                     The Black Maria  Â£52.15       In stock\n",
      "10     Starving Hearts (Triangular Trade Trilogy, #1)  Â£13.99       In stock\n",
      "11                              Shakespeare's Sonnets  Â£20.66       In stock\n",
      "12                                        Set Me Free  Â£17.46       In stock\n",
      "13  Scott Pilgrim's Precious Little Life (Scott Pi...  Â£52.29       In stock\n",
      "14                          Rip it Up and Start Again  Â£35.02       In stock\n",
      "15  Our Band Could Be Your Life: Scenes from the A...  Â£57.25       In stock\n",
      "16                                               Olio  Â£23.88       In stock\n",
      "17  Mesaerion: The Best Science Fiction Stories 18...  Â£37.59       In stock\n",
      "18                       Libertarianism for Beginners  Â£51.33       In stock\n",
      "19                            It's Only the Himalayas  Â£45.17       In stock\n"
     ]
    }
   ],
   "source": [
    "def scrape_books(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    book_titles = []\n",
    "    book_prices = []\n",
    "    book_availability = []\n",
    "\n",
    "    titles = soup.find_all('h3')\n",
    "    for title in titles:\n",
    "        book_titles.append(title.find('a')['title'])\n",
    "\n",
    "    prices = soup.find_all('p', class_='price_color')\n",
    "    for price in prices:\n",
    "        book_prices.append(price.text)\n",
    "\n",
    "    availability = soup.find_all('p', class_='instock availability')\n",
    "    for avail in availability:\n",
    "        book_availability.append(avail.text.strip())\n",
    "\n",
    "    df = pd.DataFrame({'Nombre del Libro': book_titles, 'Precio': book_prices, 'Disponibilidad': book_availability})\n",
    "\n",
    "    return df\n",
    "\n",
    "url14 = 'http://books.toscrape.com/'\n",
    "\n",
    "book_df = scrape_books(url14)\n",
    "\n",
    "print(book_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitates tu output? Gracias! ðŸ™‚**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
